{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../config/multi_task_cross_stitch.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "sys.argv = [\"view\", \"--config\", \"../../config/multi_task_cross_stitch.yaml\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import VOC08Attr\n",
    "from torchvision.transforms import transforms\n",
    "from config_experiments import config\n",
    "from model import ObjectDetectionModel, AttributePredictionModel, CrossStitchNet\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.onnx\n",
    "import netron\n",
    "from utils import set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data01/dl23vitcas/dl_project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=config[\"transform\"][\"resize_values\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=config[\"transform\"][\"mean\"], std=config[\"transform\"][\"std\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# FOR VGG16\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m path_best_model_obj \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_obj\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m path_best_model_attr \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_attr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m model_obj \u001b[38;5;241m=\u001b[39m ObjectDetectionModel()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR VGG16\n",
    "\n",
    "path_best_model_obj = config[\"model\"][\"model_obj\"]\n",
    "path_best_model_attr = config[\"model\"][\"model_attr\"]\n",
    "model_obj = ObjectDetectionModel().to(device)\n",
    "model_attr = AttributePredictionModel().to(device)\n",
    "model_cross = CrossStitchNet(model_obj.backbone, model_attr.backbone)\n",
    "\n",
    "for i, (name, params) in enumerate(model_cross.named_parameters()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "best_model_obj = torch.load(path_best_model_obj, map_location=device)\n",
    "\n",
    "for i, (name, params) in enumerate(best_model_obj.items()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "best_model_attr = torch.load(path_best_model_attr, map_location=device)\n",
    "\n",
    "for i, (name, params) in enumerate(best_model_attr.items()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "mapping_obj = {\n",
    "    \"cross_stitch_net.models_a.0.0.weight\": \"alex.features.0.weight\",\n",
    "    \"cross_stitch_net.models_a.0.0.bias\": \"alex.features.0.bias\",\n",
    "    \"cross_stitch_net.models_a.0.2.weight\": \"alex.features.2.weight\",\n",
    "    \"cross_stitch_net.models_a.0.2.bias\": \"alex.features.2.bias\",\n",
    "    \"cross_stitch_net.models_a.1.0.weight\": \"alex.features.5.weight\",\n",
    "    \"cross_stitch_net.models_a.1.0.bias\": \"alex.features.5.bias\",\n",
    "    \"cross_stitch_net.models_a.1.2.weight\": \"alex.features.7.weight\",\n",
    "    \"cross_stitch_net.models_a.1.2.bias\": \"alex.features.7.bias\",\n",
    "    \"cross_stitch_net.models_a.2.0.weight\": \"alex.features.10.weight\",\n",
    "    \"cross_stitch_net.models_a.2.0.bias\": \"alex.features.10.bias\",\n",
    "    \"cross_stitch_net.models_a.2.2.weight\": \"alex.features.12.weight\",\n",
    "    \"cross_stitch_net.models_a.2.2.bias\": \"alex.features.12.bias\",\n",
    "    \"cross_stitch_net.models_a.2.4.weight\": \"alex.features.14.weight\",\n",
    "    \"cross_stitch_net.models_a.2.4.bias\": \"alex.features.14.bias\",\n",
    "    \"cross_stitch_net.models_a.3.0.weight\": \"alex.features.17.weight\",\n",
    "    \"cross_stitch_net.models_a.3.0.bias\": \"alex.features.17.bias\",\n",
    "    \"cross_stitch_net.models_a.3.2.weight\": \"alex.features.19.weight\",\n",
    "    \"cross_stitch_net.models_a.3.2.bias\": \"alex.features.19.bias\",\n",
    "    \"cross_stitch_net.models_a.3.4.weight\": \"alex.features.21.weight\",\n",
    "    \"cross_stitch_net.models_a.3.4.bias\": \"alex.features.21.bias\",\n",
    "    \"cross_stitch_net.models_a.4.0.weight\": \"alex.features.24.weight\",\n",
    "    \"cross_stitch_net.models_a.4.0.bias\": \"alex.features.24.bias\",\n",
    "    \"cross_stitch_net.models_a.4.2.weight\": \"alex.features.26.weight\",\n",
    "    \"cross_stitch_net.models_a.4.2.bias\": \"alex.features.26.bias\",\n",
    "    \"cross_stitch_net.models_a.4.4.weight\": \"alex.features.28.weight\",\n",
    "    \"cross_stitch_net.models_a.4.4.bias\": \"alex.features.28.bias\",\n",
    "    \"cross_stitch_classifier.branch_a.1.weight\": \"roi_module.classifier.1.weight\",\n",
    "    \"cross_stitch_classifier.branch_a.1.bias\": \"roi_module.classifier.1.bias\",\n",
    "    \"cross_stitch_classifier.branch_a.4.weight\": \"roi_module.classifier.4.weight\",\n",
    "    \"cross_stitch_classifier.branch_a.4.bias\": \"roi_module.classifier.4.bias\",\n",
    "    \"model_obj_detect.cls_score.weight\": \"obj_detect_head.cls_score.weight\",\n",
    "    \"model_obj_detect.cls_score.bias\": \"obj_detect_head.cls_score.bias\",\n",
    "    \"model_obj_detect.bbox.weight\": \"obj_detect_head.bbox.weight\",\n",
    "    \"model_obj_detect.bbox.bias\": \"obj_detect_head.bbox.bias\",\n",
    "}\n",
    "\n",
    "mapping_attr = {\n",
    "    \"cross_stitch_net.models_b.0.0.weight\": \"alex.features.0.weight\",\n",
    "    \"cross_stitch_net.models_b.0.0.bias\": \"alex.features.0.bias\",\n",
    "    \"cross_stitch_net.models_b.0.2.weight\": \"alex.features.2.weight\",\n",
    "    \"cross_stitch_net.models_b.0.2.bias\": \"alex.features.2.bias\",\n",
    "    \"cross_stitch_net.models_b.1.0.weight\": \"alex.features.5.weight\",\n",
    "    \"cross_stitch_net.models_b.1.0.bias\": \"alex.features.5.bias\",\n",
    "    \"cross_stitch_net.models_b.1.2.weight\": \"alex.features.7.weight\",\n",
    "    \"cross_stitch_net.models_b.1.2.bias\": \"alex.features.7.bias\",\n",
    "    \"cross_stitch_net.models_b.2.0.weight\": \"alex.features.10.weight\",\n",
    "    \"cross_stitch_net.models_b.2.0.bias\": \"alex.features.10.bias\",\n",
    "    \"cross_stitch_net.models_b.2.2.weight\": \"alex.features.12.weight\",\n",
    "    \"cross_stitch_net.models_b.2.2.bias\": \"alex.features.12.bias\",\n",
    "    \"cross_stitch_net.models_b.2.4.weight\": \"alex.features.14.weight\",\n",
    "    \"cross_stitch_net.models_b.2.4.bias\": \"alex.features.14.bias\",\n",
    "    \"cross_stitch_net.models_b.3.0.weight\": \"alex.features.17.weight\",\n",
    "    \"cross_stitch_net.models_b.3.0.bias\": \"alex.features.17.bias\",\n",
    "    \"cross_stitch_net.models_b.3.2.weight\": \"alex.features.19.weight\",\n",
    "    \"cross_stitch_net.models_b.3.2.bias\": \"alex.features.19.bias\",\n",
    "    \"cross_stitch_net.models_b.3.4.weight\": \"alex.features.21.weight\",\n",
    "    \"cross_stitch_net.models_b.3.4.bias\": \"alex.features.21.bias\",\n",
    "    \"cross_stitch_net.models_b.4.0.weight\": \"alex.features.24.weight\",\n",
    "    \"cross_stitch_net.models_b.4.0.bias\": \"alex.features.24.bias\",\n",
    "    \"cross_stitch_net.models_b.4.2.weight\": \"alex.features.26.weight\",\n",
    "    \"cross_stitch_net.models_b.4.2.bias\": \"alex.features.26.bias\",\n",
    "    \"cross_stitch_net.models_b.4.4.weight\": \"alex.features.28.weight\",\n",
    "    \"cross_stitch_net.models_b.4.4.bias\": \"alex.features.28.bias\",\n",
    "    \"cross_stitch_classifier.branch_b.1.weight\": \"roi_module.classifier.1.weight\",\n",
    "    \"cross_stitch_classifier.branch_b.1.bias\": \"roi_module.classifier.1.bias\",\n",
    "    \"cross_stitch_classifier.branch_b.4.weight\": \"roi_module.classifier.4.weight\",\n",
    "    \"cross_stitch_classifier.branch_b.4.bias\": \"roi_module.classifier.4.bias\",\n",
    "    \"model_attribute.attr_score.weight\": \"attribute_head.attr_score.weight\",\n",
    "    \"model_attribute.attr_score.bias\": \"attribute_head.attr_score.bias\",\n",
    "}\n",
    "\n",
    "# Copia dei pesi per il task di object detection\n",
    "for name, param in model_cross.named_parameters():\n",
    "    if name in mapping_obj:\n",
    "        source_name = mapping_obj[name]\n",
    "        param.data.copy_(best_model_obj[source_name].data)\n",
    "\n",
    "# Copia dei pesi per il task di attribute classification\n",
    "for name, param in model_cross.named_parameters():\n",
    "    if name in mapping_attr:\n",
    "        source_name = mapping_attr[name]\n",
    "        param.data.copy_(best_model_attr[source_name].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ALEXNET\n",
    "\n",
    "path_best_model_obj = \"experiments/object_detection/2024-07-24_09-59-58/models/best_model_epoch_93.pth\"  # 2024-07-24_09-59-58\n",
    "path_best_model_attr = \"experiments/attribute_prediction/2024-08-02_11-55-19/models/best_model_epoch_60.pth\"  # 2024-08-02_11-55-19\n",
    "model_obj = ObjectDetectionModel().to(device)\n",
    "model_attr = AttributePredictionModel().to(device)\n",
    "model_cross = CrossStitchNet(model_obj.backbone, model_attr.backbone)\n",
    "\n",
    "for i, (name, params) in enumerate(model_cross.named_parameters()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "best_model_obj = torch.load(path_best_model_obj, map_location=device)\n",
    "\n",
    "for i, (name, params) in enumerate(best_model_obj.items()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "best_model_attr = torch.load(path_best_model_attr, map_location=device)\n",
    "\n",
    "for i, (name, params) in enumerate(best_model_attr.items()):\n",
    "    print(i, name, params.shape)\n",
    "\n",
    "# Caricamento dei modelli single-task\n",
    "best_model_obj = torch.load(path_best_model_obj, map_location=device)\n",
    "best_model_attr = torch.load(path_best_model_attr, map_location=device)\n",
    "\n",
    "# Mappa tra i nomi dei layer nei modelli single-task e multi-task\n",
    "mapping_obj = {\n",
    "    \"cross_stitch_net.models_a.0.0.weight\": \"alex.features.0.weight\",\n",
    "    \"cross_stitch_net.models_a.0.0.bias\": \"alex.features.0.bias\",\n",
    "    \"cross_stitch_net.models_a.1.0.weight\": \"alex.features.3.weight\",\n",
    "    \"cross_stitch_net.models_a.1.0.bias\": \"alex.features.3.bias\",\n",
    "    \"cross_stitch_net.models_a.2.0.weight\": \"alex.features.6.weight\",\n",
    "    \"cross_stitch_net.models_a.2.0.bias\": \"alex.features.6.bias\",\n",
    "    \"cross_stitch_net.models_a.2.2.weight\": \"alex.features.8.weight\",\n",
    "    \"cross_stitch_net.models_a.2.2.bias\": \"alex.features.8.bias\",\n",
    "    \"cross_stitch_net.models_a.2.4.weight\": \"alex.features.10.weight\",\n",
    "    \"cross_stitch_net.models_a.2.4.bias\": \"alex.features.10.bias\",\n",
    "    \"cross_stitch_classifier.branch_a.1.weight\": \"roi_module.classifier.1.weight\",\n",
    "    \"cross_stitch_classifier.branch_a.1.bias\": \"roi_module.classifier.1.bias\",\n",
    "    \"cross_stitch_classifier.branch_a.4.weight\": \"roi_module.classifier.4.weight\",\n",
    "    \"cross_stitch_classifier.branch_a.4.bias\": \"roi_module.classifier.4.bias\",\n",
    "    \"model_obj_detect.cls_score.weight\": \"obj_detect_head.cls_score.weight\",\n",
    "    \"model_obj_detect.cls_score.bias\": \"obj_detect_head.cls_score.bias\",\n",
    "    \"model_obj_detect.bbox.weight\": \"obj_detect_head.bbox.weight\",\n",
    "    \"model_obj_detect.bbox.bias\": \"obj_detect_head.bbox.bias\",\n",
    "}\n",
    "\n",
    "mapping_attr = {\n",
    "    \"cross_stitch_net.models_b.0.0.weight\": \"alex.features.0.weight\",\n",
    "    \"cross_stitch_net.models_b.0.0.bias\": \"alex.features.0.bias\",\n",
    "    \"cross_stitch_net.models_b.1.0.weight\": \"alex.features.3.weight\",\n",
    "    \"cross_stitch_net.models_b.1.0.bias\": \"alex.features.3.bias\",\n",
    "    \"cross_stitch_net.models_b.2.0.weight\": \"alex.features.6.weight\",\n",
    "    \"cross_stitch_net.models_b.2.0.bias\": \"alex.features.6.bias\",\n",
    "    \"cross_stitch_net.models_b.2.2.weight\": \"alex.features.8.weight\",\n",
    "    \"cross_stitch_net.models_b.2.2.bias\": \"alex.features.8.bias\",\n",
    "    \"cross_stitch_net.models_b.2.4.weight\": \"alex.features.10.weight\",\n",
    "    \"cross_stitch_net.models_b.2.4.bias\": \"alex.features.10.bias\",\n",
    "    \"cross_stitch_classifier.branch_b.1.weight\": \"roi_module.classifier.1.weight\",\n",
    "    \"cross_stitch_classifier.branch_b.1.bias\": \"roi_module.classifier.1.bias\",\n",
    "    \"cross_stitch_classifier.branch_b.4.weight\": \"roi_module.classifier.4.weight\",\n",
    "    \"cross_stitch_classifier.branch_b.4.bias\": \"roi_module.classifier.4.bias\",\n",
    "    \"model_attribute.attr_score.weight\": \"attribute_head.attr_score.weight\",\n",
    "    \"model_attribute.attr_score.bias\": \"attribute_head.attr_score.bias\",\n",
    "}\n",
    "\n",
    "\n",
    "for name, param in model_cross.named_parameters():\n",
    "    if name in mapping_obj:\n",
    "        source_name = mapping_obj[name]\n",
    "        param.data.copy_(best_model_obj[source_name].data)\n",
    "\n",
    "for name, param in model_cross.named_parameters():\n",
    "    if name in mapping_attr:\n",
    "        source_name = mapping_attr[name]\n",
    "        param.data.copy_(best_model_attr[source_name].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
