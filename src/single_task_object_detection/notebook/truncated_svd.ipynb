{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data01/dl23vitcas/dl_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config/single_task_object_detection.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "sys.argv = [\"view\", \"--config\", \"config/single_task_object_detection.yaml\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "W = torch.arange(12, dtype=torch.float32).reshape(4, 3)\n",
    "b = torch.arange(4, dtype=torch.float32)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0835,  0.8325,  0.5319],\n",
      "        [-0.3137,  0.4490, -0.8066],\n",
      "        [-0.5438,  0.0656,  0.0176],\n",
      "        [-0.7739, -0.3179,  0.2571]])\n",
      "tensor([2.2447e+01, 1.4641e+00, 5.6755e-07])\n",
      "tensor([[2.2447e+01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4641e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 5.6755e-07]])\n",
      "tensor([[-0.4976, -0.7653, -0.4082],\n",
      "        [-0.5740, -0.0624,  0.8165],\n",
      "        [-0.6504,  0.6406, -0.4082]])\n"
     ]
    }
   ],
   "source": [
    "print(U)\n",
    "print(S)\n",
    "print(torch.diag(S))\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9605e-08, 1.0000e+00, 2.0000e+00],\n",
       "        [3.0000e+00, 4.0000e+00, 5.0000e+00],\n",
       "        [6.0000e+00, 7.0000e+00, 8.0000e+00],\n",
       "        [9.0000e+00, 1.0000e+01, 1.1000e+01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U @ torch.diag(S) @ V.t()  # deve essere un'approssimazione di W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2\n",
    "St = S[:t]\n",
    "St_diag = torch.diag(St)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22.4467,  1.4641])\n",
      "tensor([[22.4467,  0.0000],\n",
      "        [ 0.0000,  1.4641]])\n"
     ]
    }
   ],
   "source": [
    "print(St)\n",
    "print(St_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22.4467,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.4641,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# pad: left, right, up, down\n",
    "# aggiunge un numero di colonne e righe di padding specificato  in questo caso solo una a destra e una sotto\n",
    "St_diag = F.pad(input=St_diag, pad=(0, 1, 0, 1), mode=\"constant\", value=0)\n",
    "print(St_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7881e-07, 1.0000e+00, 2.0000e+00],\n",
       "        [3.0000e+00, 4.0000e+00, 5.0000e+00],\n",
       "        [6.0000e+00, 7.0000e+00, 8.0000e+00],\n",
       "        [9.0000e+00, 1.0000e+01, 1.1000e+01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U @ St_diag @ V.t()  # deve essere un'approssimazione di W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compress a network, the single fully connected layer corresponding to W is replaced by two fully connected layers, without a non-linearity between them. The first of these layers uses the weight matrix Î£tV T (and no biases) and the second uses U (with the original bi- ases associated with W ). This simple compression method gives good speedups when the number of RoIs is large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change weights matrix to FC layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.arange(12, dtype=torch.float32).reshape(4, 3)\n",
    "b = torch.arange(4, dtype=torch.float32)\n",
    "fc = torch.nn.Linear(3, 4, bias=True)\n",
    "fc.weight = torch.nn.Parameter(W)\n",
    "fc.bias = torch.nn.Parameter(b)\n",
    "print(fc.weight)\n",
    "print(fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(fc.weight)\n",
    "t = 2\n",
    "St = S[:t]\n",
    "St_diag = torch.diag(St)\n",
    "St_diag = F.pad(input=St_diag, pad=(0, 1, 0, 1), mode=\"constant\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesi: Parameter containing:\n",
      "tensor([[-11.1689, -12.8838, -14.5986],\n",
      "        [ -1.1205,  -0.0913,   0.9379],\n",
      "        [  0.0000,   0.0000,   0.0000]], requires_grad=True)\n",
      "Bias: None\n"
     ]
    }
   ],
   "source": [
    "desired_weights = St_diag @ V.t()\n",
    "input_size = desired_weights.size(1)\n",
    "output_size = desired_weights.size(0)\n",
    "first_fc_layer = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    first_fc_layer.weight = nn.Parameter(desired_weights)\n",
    "\n",
    "print(\"Pesi:\", first_fc_layer.weight)\n",
    "print(\"Bias:\", first_fc_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesi: Parameter containing:\n",
      "tensor([[-0.0835,  0.8325,  0.5319],\n",
      "        [-0.3137,  0.4490, -0.8066],\n",
      "        [-0.5438,  0.0656,  0.0176],\n",
      "        [-0.7739, -0.3179,  0.2571]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([0., 1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "desired_weights = U\n",
    "desired_bias = fc.bias\n",
    "\n",
    "input_size = desired_weights.size(1)\n",
    "output_size = desired_weights.size(0)\n",
    "fc_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fc_layer.weight = nn.Parameter(desired_weights)\n",
    "    fc_layer.bias = nn.Parameter(desired_bias)\n",
    "\n",
    "# Stampa pesi e bias per verificare\n",
    "print(\"Pesi:\", fc_layer.weight)\n",
    "print(\"Bias:\", fc_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def truncated_svd_decomposition(layer, t):\n",
    "    \"\"\"\n",
    "    Applies truncated SVD decomposition on the given linear layer.\n",
    "\n",
    "    Args:\n",
    "    - layer (nn.Linear): The linear layer to be decomposed.\n",
    "    - t (int): Number of singular values to keep.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Sequential: A sequential model with two linear layers representing the truncated SVD.\n",
    "    \"\"\"\n",
    "    # Perform SVD on the weight matrix\n",
    "    W = layer.weight.data\n",
    "    U, S, V = torch.svd(W)\n",
    "\n",
    "    # Keep only the top t singular values\n",
    "    U_t = U[:, :t]\n",
    "    S_t = S[:t]\n",
    "    V_t = V[:, :t]\n",
    "\n",
    "    # Create the two new linear layers\n",
    "    first_layer = nn.Linear(V_t.shape[0], t, bias=False)\n",
    "    second_layer = nn.Linear(t, U_t.shape[0], bias=True)\n",
    "\n",
    "    # Initialize the weights of the new layers\n",
    "    first_layer.weight.data = (S_t.unsqueeze(1) * V_t.t()).t()\n",
    "    second_layer.weight.data = U_t.t()\n",
    "\n",
    "    # Set the bias of the second layer to be the same as the original layer\n",
    "    second_layer.bias.data = layer.bias.data.clone()\n",
    "\n",
    "    # Return a sequential model of the two layers\n",
    "    return nn.Sequential(first_layer, second_layer)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "original_layer = nn.Linear(4096, 4096)\n",
    "compressed_layer = truncated_svd_decomposition(original_layer, t=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.1849e-03,  3.7905e-03, -1.1864e-02,  6.9797e-03, -5.0416e-03,\n",
       "          2.3502e-04,  1.8620e-03, -1.3436e-02, -1.2332e-02, -1.5618e-02,\n",
       "          7.6030e-03,  4.2766e-03, -1.4970e-02,  2.1668e-03,  1.4707e-02,\n",
       "          3.9622e-03, -9.1828e-03,  9.1545e-03, -1.4930e-02, -2.9909e-03],\n",
       "        [-2.4309e-03, -1.6240e-03,  9.8420e-03, -1.3323e-02, -6.3613e-03,\n",
       "          9.7429e-03, -4.2976e-03, -1.1442e-02,  7.0483e-03, -5.5832e-03,\n",
       "         -5.4922e-03, -3.5614e-03, -4.0714e-03, -2.5810e-04, -1.3228e-03,\n",
       "         -4.2175e-03, -5.9025e-04,  1.4878e-02, -4.6300e-03,  1.3859e-02],\n",
       "        [-6.3232e-03,  4.9718e-03, -3.0489e-03,  5.1171e-03, -1.0547e-02,\n",
       "          8.3346e-03,  1.1239e-02, -7.6160e-03, -4.8224e-04,  8.3424e-03,\n",
       "         -1.3555e-02, -1.0539e-03,  5.3711e-03, -4.5070e-03,  2.2237e-03,\n",
       "          1.5609e-02,  1.3556e-02,  1.8068e-03,  8.0590e-03,  1.3183e-02],\n",
       "        [ 5.8181e-03,  1.7129e-03, -5.5282e-03, -7.2097e-03, -1.2825e-02,\n",
       "          5.1431e-04, -9.8447e-03,  1.5290e-02,  3.1371e-04, -4.9471e-03,\n",
       "         -1.6249e-03, -1.9527e-03, -5.8365e-03, -8.7370e-03, -2.2799e-03,\n",
       "          3.0057e-04,  4.2286e-03, -4.2122e-03, -4.2039e-03, -1.1407e-02],\n",
       "        [ 1.9987e-03, -1.5427e-02,  1.2589e-02, -2.5074e-03,  3.3811e-03,\n",
       "          3.5662e-03,  6.3268e-03,  1.0503e-02,  1.1514e-02, -8.7681e-03,\n",
       "          3.3586e-03, -7.4425e-03, -1.0303e-02,  1.1204e-02,  1.4633e-02,\n",
       "          8.8145e-03, -3.8760e-03,  9.2051e-03, -2.2065e-03,  5.6405e-03],\n",
       "        [-2.3565e-03,  2.0870e-03, -1.1054e-02,  1.4048e-02, -1.3884e-02,\n",
       "          9.8649e-03, -1.4454e-02, -4.1089e-03,  8.2492e-03, -4.9401e-03,\n",
       "          1.5129e-02,  2.7648e-03, -2.1868e-03, -1.2926e-03,  1.0448e-02,\n",
       "          1.4928e-02,  1.4912e-02,  1.2511e-02,  1.0342e-02, -1.1599e-02],\n",
       "        [-4.1401e-03,  3.0748e-03,  4.6990e-03,  1.3583e-02,  9.8205e-03,\n",
       "          7.9903e-03,  1.5542e-02,  4.9697e-03,  8.2989e-03, -1.0660e-04,\n",
       "          8.8009e-03, -6.8670e-03,  5.3316e-03, -1.0564e-02, -7.7200e-03,\n",
       "          8.0066e-04,  2.6018e-03,  6.1607e-05, -5.6592e-03,  9.7953e-03],\n",
       "        [ 2.5656e-03, -5.1118e-03, -1.1665e-02, -1.5397e-02, -1.0295e-02,\n",
       "          6.0319e-03,  4.4679e-03, -1.7487e-03,  1.1108e-02, -2.6608e-03,\n",
       "         -9.6411e-03,  1.5360e-02, -1.0668e-04,  3.3542e-03, -4.5963e-03,\n",
       "         -2.4042e-03, -1.7837e-03,  8.0330e-03,  1.2595e-02, -2.9129e-03],\n",
       "        [ 3.7735e-03,  1.4466e-02,  1.2089e-02, -4.4173e-03,  9.3311e-03,\n",
       "          3.9969e-03,  2.3554e-03,  1.0352e-02, -3.4342e-03, -5.3575e-03,\n",
       "          1.2431e-02,  3.1759e-03, -1.2157e-02,  2.6329e-03, -9.0176e-03,\n",
       "          8.8811e-03, -2.6530e-03, -7.6426e-03,  2.6531e-03,  1.3314e-02],\n",
       "        [-1.0965e-02,  1.5678e-03,  4.4402e-03, -1.0643e-02,  1.4999e-02,\n",
       "          5.8121e-03,  1.0296e-02, -5.4526e-03, -3.9011e-03,  8.7466e-03,\n",
       "          4.6640e-03, -2.7595e-03,  8.8169e-03,  8.2562e-03,  5.6732e-03,\n",
       "         -1.5486e-02, -1.6153e-03,  3.6311e-03,  1.1052e-02,  5.8573e-03],\n",
       "        [ 5.5681e-03, -2.5006e-03,  1.4982e-02, -1.2825e-02,  1.4819e-02,\n",
       "          1.5359e-03,  7.7891e-03, -1.4179e-02, -9.9510e-03, -1.2264e-02,\n",
       "         -3.2896e-03,  4.1591e-03, -3.6761e-03,  4.3515e-03,  8.4868e-03,\n",
       "          4.4904e-03, -1.1472e-02,  1.3898e-02,  5.0048e-03,  1.5093e-02],\n",
       "        [-2.7266e-03,  1.6883e-03,  7.9219e-03, -4.1181e-03, -1.2705e-02,\n",
       "          3.2290e-03,  1.1592e-02, -4.0424e-03, -9.2026e-03,  1.1250e-02,\n",
       "          1.1782e-03,  1.5337e-02, -5.1806e-03, -5.0766e-03, -1.1582e-02,\n",
       "          4.4186e-03,  1.1192e-02, -1.4929e-02, -1.4737e-02, -5.6828e-03],\n",
       "        [-1.5603e-02, -1.2231e-02,  6.3651e-03,  1.3192e-02, -8.1778e-03,\n",
       "         -3.3935e-03, -3.3771e-04,  1.1739e-02, -3.3956e-03, -2.5134e-03,\n",
       "         -1.1519e-02, -9.8198e-04,  1.4270e-02,  7.2378e-04,  1.1552e-02,\n",
       "          1.3415e-02,  3.1321e-03,  3.6723e-03,  9.6096e-03, -1.5563e-02],\n",
       "        [-2.2746e-03,  1.2971e-02, -3.4543e-03, -1.1195e-02, -6.8760e-03,\n",
       "          1.5082e-02, -1.5222e-02, -2.6633e-04,  1.1526e-02,  5.9766e-03,\n",
       "         -4.4170e-03,  7.1358e-03,  5.3631e-03,  1.4776e-02,  8.1876e-03,\n",
       "         -1.5253e-02,  1.3918e-03,  9.9989e-03, -1.1419e-02, -1.0415e-02],\n",
       "        [-7.6309e-04, -9.7987e-03, -1.2825e-02, -1.0255e-02, -1.3974e-02,\n",
       "          2.9750e-03,  1.3447e-02, -1.1934e-02, -2.6705e-03, -7.9275e-03,\n",
       "          7.9387e-03, -6.9258e-04, -1.3053e-02,  1.9828e-03,  1.5605e-02,\n",
       "         -4.9099e-03,  3.9913e-03,  1.4968e-02,  4.3281e-03,  1.2296e-03],\n",
       "        [-1.1507e-02, -1.2730e-04,  1.1962e-02, -1.0032e-02, -1.1295e-03,\n",
       "          8.0470e-03,  8.6692e-03, -1.3775e-02, -1.5118e-02,  2.7075e-03,\n",
       "          1.1764e-02,  6.9974e-03,  5.0824e-04, -3.7961e-03, -5.8077e-04,\n",
       "          2.1229e-03,  2.2118e-03,  1.4348e-02,  1.5280e-02,  5.3570e-03],\n",
       "        [-2.4632e-03, -1.6476e-03, -6.1608e-03, -7.2409e-03, -1.3196e-02,\n",
       "          1.1399e-02,  1.1466e-02,  1.1676e-02, -4.7947e-03,  1.0805e-02,\n",
       "          6.7693e-03, -1.8592e-03,  8.6598e-03, -6.6081e-03, -1.0694e-03,\n",
       "         -3.9932e-04, -5.2735e-03,  6.8883e-04,  1.4304e-02,  1.2006e-02],\n",
       "        [-7.4913e-03,  8.1535e-03, -2.3750e-03,  1.1292e-02, -1.0794e-02,\n",
       "         -4.8156e-03,  1.1457e-02,  1.3356e-04, -3.2767e-03,  8.8115e-04,\n",
       "         -9.0092e-03, -7.7951e-03,  4.6562e-03,  1.2064e-02, -5.5971e-03,\n",
       "          6.6335e-03,  6.3165e-03, -1.2322e-02, -1.5244e-02, -8.5429e-03],\n",
       "        [-4.7321e-04,  5.1566e-03, -6.5985e-03, -1.1120e-02,  1.0994e-02,\n",
       "          6.2250e-03, -1.4428e-02, -1.1699e-02,  6.6094e-03, -1.6046e-03,\n",
       "          1.4741e-02,  1.1594e-02,  6.4416e-03, -1.2255e-02,  5.9840e-03,\n",
       "          1.3715e-02,  6.6428e-03, -6.8155e-03, -1.4047e-02,  5.7636e-03],\n",
       "        [ 1.1117e-02,  9.7981e-03, -1.4066e-02, -8.6090e-03,  1.2995e-02,\n",
       "         -6.5401e-04,  8.4137e-03,  4.6908e-03,  1.0454e-03, -1.4203e-02,\n",
       "         -1.0385e-02,  1.3937e-02,  7.4744e-03, -3.6909e-03,  1.5466e-02,\n",
       "          5.8275e-03, -3.7890e-04, -8.0078e-03, -4.7335e-03, -1.4184e-02]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_layer.weight[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ObjectDetectionModel as ObjModelFull\n",
    "from model_compressed import ObjectDetectionModel as ObjModelCompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Conta il numero di parametri nel modello.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): Il modello PyTorch di cui contare i parametri.\n",
    "\n",
    "    Returns:\n",
    "    - int: Il numero totale di parametri nel modello.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116065169"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(ObjModelFull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67051738"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(ObjModelCompressed())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
