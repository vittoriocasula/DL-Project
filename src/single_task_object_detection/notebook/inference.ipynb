{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "sys.argv = [\"view\", \"--config\", \"config/single_task_object_detection.yaml\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from config_experiments import config\n",
    "from dataloader import VOC08Attr\n",
    "from bbox_transform import resize_bounding_boxes\n",
    "\n",
    "device = \"cuda\"\n",
    "from bbox_transform import (\n",
    "    apply_nms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)  # (H, W)\n",
    "        axs[0, i].imshow(np.asarray(img))  # (W, H, 3)\n",
    "        # axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]) # uncomment to remove axis in plot\n",
    "\n",
    "\n",
    "def show_bounding_box(img_tensor, box, labels=None, color=\"white\"):\n",
    "    img_tensor = inverse_transform(img_tensor)\n",
    "    show(draw_bounding_boxes(img_tensor, box, colors=color, labels=labels, width=1))\n",
    "\n",
    "\n",
    "def show_bbox_with_transform(img, box, mean, std, labels=None, color=\"white\"):\n",
    "    img = inverse_transorm_with_normalize(img, mean, std)\n",
    "    show(draw_bounding_boxes(img, box, colors=color, labels=labels, width=2))\n",
    "\n",
    "\n",
    "def inverse_transorm_with_normalize(image, mean, std):\n",
    "    image = np.array(image)\n",
    "    for channel in range(3):\n",
    "        image[channel] = image[channel] * std[channel] + mean[channel]\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def inverse_transform(image):\n",
    "    image = np.array(image)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ObjectDetectionModel\n",
    "\n",
    "model_path = \"models/object_detection/model_2024-06-18_11-08.pth\"\n",
    "model = ObjectDetectionModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            size=config[\"transform\"][\"resize_values\"],\n",
    "            max_size=config[\"transform\"][\"max_size\"],\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=config[\"transform\"][\"mean\"], std=config[\"transform\"][\"std\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# dataset & dataloader\n",
    "\n",
    "val_data = VOC08Attr(train=False, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Funzione per denormalizzare l'immagine\n",
    "def denormalize(image, mean, std):\n",
    "    image = image.clone()\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Funzione per visualizzare l'immagine con i bounding box\n",
    "def show_image_with_boxes(image_tensor, boxes, mean, std):\n",
    "    # Denormalizzare l'immagine\n",
    "    image = denormalize(image_tensor, mean, std)\n",
    "    image = image.numpy().transpose((1, 2, 0))  # Convertire da (C, H, W) a (H, W, C)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Aggiungere i bounding box\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height, linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def resize_image(image, orig_size):\n",
    "    return F.interpolate(\n",
    "        image.unsqueeze(0), size=orig_size, mode=\"bilinear\", align_corners=False\n",
    "    ).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\n",
    "    os.getcwd()\n",
    "    + \"/src/single_task_object_detection/\"\n",
    "    + \"target_mean_std_by_class.yaml\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    mean_std_by_class = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randint(0, 100, (1,)).item()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image, image_size, gt_class, gt_bbox, gt_attributes, ss_rois = val_data[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    gt_class = gt_class.to(device)\n",
    "    gt_bbox = gt_bbox.to(device)\n",
    "    ss_rois = ss_rois.to(device)\n",
    "    orig_w, orig_h = image_size\n",
    "    new_w, new_h = (image.shape[3], image.shape[2])\n",
    "    gt_bbox = resize_bounding_boxes(\n",
    "        gt_bbox, orig_size=(new_w, new_h), new_size=(orig_w, orig_h)\n",
    "    )\n",
    "\n",
    "    indices_batch = val_data.get_indices_batch(\n",
    "        image.shape[0], ss_rois.shape[0]\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    indices_batch = indices_batch.to(device)\n",
    "\n",
    "    cls_max_score_net, max_score_net, bboxs_net = model.prediction_img(\n",
    "        image, ss_rois, indices_batch, mean_std_by_class\n",
    "    )\n",
    "\n",
    "    pred_bbox, pred_class, pred_score = apply_nms(\n",
    "        cls_max_score_net, max_score_net, bboxs_net\n",
    "    )\n",
    "\n",
    "    pred_bbox = resize_bounding_boxes(\n",
    "        pred_bbox, orig_size=(new_w, new_h), new_size=(orig_w, orig_h)\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"GROUND TRUTH\")\n",
    "print(gt_bbox)\n",
    "print([val_data.id2category[classe.item()] for classe in gt_class])\n",
    "print(\"\\nPREDICTION\\n\")\n",
    "print(pred_bbox)\n",
    "print([val_data.id2category[classe.item()] for classe in pred_class])\n",
    "print(pred_score)\n",
    "\n",
    "original_image = resize_image(image.squeeze().cpu(), (orig_h, orig_w))\n",
    "show_image_with_boxes(\n",
    "    original_image,\n",
    "    gt_bbox.cpu(),\n",
    "    mean=config[\"transform\"][\"mean\"],\n",
    "    std=config[\"transform\"][\"std\"],\n",
    ")\n",
    "\n",
    "show_image_with_boxes(\n",
    "    original_image,\n",
    "    pred_bbox.cpu(),\n",
    "    mean=config[\"transform\"][\"mean\"],\n",
    "    std=config[\"transform\"][\"std\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
