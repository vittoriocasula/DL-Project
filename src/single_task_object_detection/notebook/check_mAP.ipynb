{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute PASCAL_VOC MAP.\n",
    "\n",
    "Reference:\n",
    "  https://github.com/chainer/chainercv/blob/master/chainercv/evaluations/eval_detection_voc.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def voc_eval(\n",
    "    pred_bboxes,\n",
    "    pred_labels,\n",
    "    pred_scores,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    gt_difficults=None,\n",
    "    iou_thresh=0.5,\n",
    "    use_07_metric=True,\n",
    "):\n",
    "    \"\"\"Wrap VOC evaluation for PyTorch.\"\"\"\n",
    "    pred_bboxes = [xy2yx(b).numpy() for b in pred_bboxes]\n",
    "    pred_labels = [label.numpy() for label in pred_labels]\n",
    "    pred_scores = [score.numpy() for score in pred_scores]\n",
    "    gt_bboxes = [xy2yx(b).numpy() for b in gt_bboxes]\n",
    "    gt_labels = [label.numpy() for label in gt_labels]\n",
    "    return eval_detection_voc(\n",
    "        pred_bboxes,\n",
    "        pred_labels,\n",
    "        pred_scores,\n",
    "        gt_bboxes,\n",
    "        gt_labels,\n",
    "        gt_difficults,\n",
    "        iou_thresh,\n",
    "        use_07_metric,\n",
    "    )\n",
    "\n",
    "\n",
    "def xy2yx(boxes):\n",
    "    \"\"\"Convert box (xmin,ymin,xmax,ymax) to (ymin,xmin,ymax,xmax).\"\"\"\n",
    "    c0 = boxes[:, 0].clone()\n",
    "    c2 = boxes[:, 2].clone()\n",
    "    boxes[:, 0] = boxes[:, 1]\n",
    "    boxes[:, 1] = c0\n",
    "    boxes[:, 2] = boxes[:, 3]\n",
    "    boxes[:, 3] = c2\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n",
    "            :math:`N` is the number of bounding boxes.\n",
    "            The dtype should be :obj:`numpy.float32`.\n",
    "        bbox_b (array): An array similar to :obj:`bbox_a`,\n",
    "            whose shape is :math:`(K, 4)`.\n",
    "            The dtype should be :obj:`numpy.float32`.\n",
    "\n",
    "    Returns:\n",
    "        array:\n",
    "        An array whose shape is :math:`(N, K)`. \\\n",
    "        An element at index :math:`(n, k)` contains IoUs between \\\n",
    "        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n",
    "        box in :obj:`bbox_b`.\n",
    "    \"\"\"\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "\n",
    "def eval_detection_voc(\n",
    "    pred_bboxes,\n",
    "    pred_labels,\n",
    "    pred_scores,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    gt_difficults=None,\n",
    "    iou_thresh=0.5,\n",
    "    use_07_metric=False,\n",
    "):\n",
    "    \"\"\"Calculate average precisions based on evaluation code of PASCAL VOC.\n",
    "\n",
    "    This function evaluates predicted bounding boxes obtained from a dataset\n",
    "    which has :math:`N` images by using average precision for each class.\n",
    "    The code is based on the evaluation code used in PASCAL VOC Challenge.\n",
    "\n",
    "    Args:\n",
    "        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n",
    "            sets of bounding boxes.\n",
    "            Its index corresponds to an index for the base dataset.\n",
    "            Each element of :obj:`pred_bboxes` is a set of coordinates\n",
    "            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n",
    "            where :math:`R` corresponds\n",
    "            to the number of bounding boxes, which may vary among boxes.\n",
    "            The second axis corresponds to\n",
    "            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n",
    "        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n",
    "            Similar to :obj:`pred_bboxes`, its index corresponds to an\n",
    "            index for the base dataset. Its length is :math:`N`.\n",
    "        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n",
    "            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n",
    "            its index corresponds to an index for the base dataset.\n",
    "            Its length is :math:`N`.\n",
    "        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n",
    "            bounding boxes\n",
    "            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n",
    "            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n",
    "            bounding boxes in each image does not need to be same as the number\n",
    "            of corresponding predicted boxes.\n",
    "        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n",
    "            labels which are organized similarly to :obj:`gt_bboxes`.\n",
    "        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n",
    "            arrays which is organized similarly to :obj:`gt_bboxes`.\n",
    "            This tells whether the\n",
    "            corresponding ground truth bounding box is difficult or not.\n",
    "            By default, this is :obj:`None`. In that case, this function\n",
    "            considers all bounding boxes to be not difficult.\n",
    "        iou_thresh (float): A prediction is correct if its Intersection over\n",
    "            Union with the ground truth is above this value.\n",
    "        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n",
    "            for calculating average precision. The default value is\n",
    "            :obj:`False`.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "\n",
    "        The keys, value-types and the description of the values are listed\n",
    "        below.\n",
    "\n",
    "        * **ap** (*numpy.ndarray*): An array of average precisions. \\\n",
    "            The :math:`l`-th value corresponds to the average precision \\\n",
    "            for class :math:`l`. If class :math:`l` does not exist in \\\n",
    "            either :obj:`pred_labels` or :obj:`gt_labels`, the corresponding \\\n",
    "            value is set to :obj:`numpy.nan`.\n",
    "        * **map** (*float*): The average of Average Precisions over classes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prec, rec = calc_detection_voc_prec_rec(\n",
    "        pred_bboxes,\n",
    "        pred_labels,\n",
    "        pred_scores,\n",
    "        gt_bboxes,\n",
    "        gt_labels,\n",
    "        gt_difficults,\n",
    "        iou_thresh=iou_thresh,\n",
    "    )\n",
    "\n",
    "    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n",
    "\n",
    "    return {\"ap\": ap, \"map\": np.nanmean(ap)}\n",
    "\n",
    "\n",
    "def calc_detection_voc_prec_rec(\n",
    "    pred_bboxes,\n",
    "    pred_labels,\n",
    "    pred_scores,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    gt_difficults=None,\n",
    "    iou_thresh=0.5,\n",
    "):\n",
    "    \"\"\"Calculate precision and recall based on evaluation code of PASCAL VOC.\n",
    "\n",
    "    This function calculates precision and recall of\n",
    "    predicted bounding boxes obtained from a dataset which has :math:`N`\n",
    "    images.\n",
    "    The code is based on the evaluation code used in PASCAL VOC Challenge.\n",
    "\n",
    "    Args:\n",
    "        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n",
    "            sets of bounding boxes.\n",
    "            Its index corresponds to an index for the base dataset.\n",
    "            Each element of :obj:`pred_bboxes` is a set of coordinates\n",
    "            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n",
    "            where :math:`R` corresponds\n",
    "            to the number of bounding boxes, which may vary among boxes.\n",
    "            The second axis corresponds to\n",
    "            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n",
    "        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n",
    "            Similar to :obj:`pred_bboxes`, its index corresponds to an\n",
    "            index for the base dataset. Its length is :math:`N`.\n",
    "        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n",
    "            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n",
    "            its index corresponds to an index for the base dataset.\n",
    "            Its length is :math:`N`.\n",
    "        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n",
    "            bounding boxes\n",
    "            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n",
    "            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n",
    "            bounding boxes in each image does not need to be same as the number\n",
    "            of corresponding predicted boxes.\n",
    "        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n",
    "            labels which are organized similarly to :obj:`gt_bboxes`.\n",
    "        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n",
    "            arrays which is organized similarly to :obj:`gt_bboxes`.\n",
    "            This tells whether the\n",
    "            corresponding ground truth bounding box is difficult or not.\n",
    "            By default, this is :obj:`None`. In that case, this function\n",
    "            considers all bounding boxes to be not difficult.\n",
    "        iou_thresh (float): A prediction is correct if its Intersection over\n",
    "            Union with the ground truth is above this value..\n",
    "\n",
    "    Returns:\n",
    "        tuple of two lists:\n",
    "        This function returns two lists: :obj:`prec` and :obj:`rec`.\n",
    "\n",
    "        * :obj:`prec`: A list of arrays. :obj:`prec[l]` is precision \\\n",
    "            for class :math:`l`. If class :math:`l` does not exist in \\\n",
    "            either :obj:`pred_labels` or :obj:`gt_labels`, :obj:`prec[l]` is \\\n",
    "            set to :obj:`None`.\n",
    "        * :obj:`rec`: A list of arrays. :obj:`rec[l]` is recall \\\n",
    "            for class :math:`l`. If class :math:`l` that is not marked as \\\n",
    "            difficult does not exist in \\\n",
    "            :obj:`gt_labels`, :obj:`rec[l]` is \\\n",
    "            set to :obj:`None`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pred_bboxes = iter(pred_bboxes)\n",
    "    pred_labels = iter(pred_labels)\n",
    "    pred_scores = iter(pred_scores)\n",
    "    gt_bboxes = iter(gt_bboxes)\n",
    "    gt_labels = iter(gt_labels)\n",
    "    if gt_difficults is None:\n",
    "        gt_difficults = itertools.repeat(None)\n",
    "    else:\n",
    "        gt_difficults = iter(gt_difficults)\n",
    "\n",
    "    n_pos = defaultdict(int)\n",
    "    score = defaultdict(list)\n",
    "    match = defaultdict(list)\n",
    "\n",
    "    for (\n",
    "        pred_bbox,\n",
    "        pred_label,\n",
    "        pred_score,\n",
    "        gt_bbox,\n",
    "        gt_label,\n",
    "        gt_difficult,\n",
    "    ) in six.moves.zip(\n",
    "        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels, gt_difficults\n",
    "    ):\n",
    "\n",
    "        if gt_difficult is None:\n",
    "            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n",
    "\n",
    "        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n",
    "            pred_mask_l = pred_label == l\n",
    "            pred_bbox_l = pred_bbox[pred_mask_l]\n",
    "            pred_score_l = pred_score[pred_mask_l]\n",
    "            # sort by score\n",
    "            order = pred_score_l.argsort()[::-1]\n",
    "            pred_bbox_l = pred_bbox_l[order]\n",
    "            pred_score_l = pred_score_l[order]\n",
    "\n",
    "            gt_mask_l = gt_label == l\n",
    "            gt_bbox_l = gt_bbox[gt_mask_l]\n",
    "            gt_difficult_l = gt_difficult[gt_mask_l]\n",
    "\n",
    "            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n",
    "            score[l].extend(pred_score_l)\n",
    "\n",
    "            if len(pred_bbox_l) == 0:\n",
    "                continue\n",
    "            if len(gt_bbox_l) == 0:\n",
    "                match[l].extend((0,) * pred_bbox_l.shape[0])\n",
    "                continue\n",
    "\n",
    "            # VOC evaluation follows integer typed bounding boxes.\n",
    "            pred_bbox_l = pred_bbox_l.copy()\n",
    "            pred_bbox_l[:, 2:] += 1\n",
    "            gt_bbox_l = gt_bbox_l.copy()\n",
    "            gt_bbox_l[:, 2:] += 1\n",
    "\n",
    "            iou = bbox_iou(pred_bbox_l, gt_bbox_l)\n",
    "            gt_index = iou.argmax(axis=1)\n",
    "            # set -1 if there is no matching ground truth\n",
    "            gt_index[iou.max(axis=1) < iou_thresh] = -1\n",
    "            del iou\n",
    "\n",
    "            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n",
    "            for gt_idx in gt_index:\n",
    "                if gt_idx >= 0:\n",
    "                    if gt_difficult_l[gt_idx]:\n",
    "                        match[l].append(-1)\n",
    "                    else:\n",
    "                        if not selec[gt_idx]:\n",
    "                            match[l].append(1)\n",
    "                        else:\n",
    "                            match[l].append(0)\n",
    "                    selec[gt_idx] = True\n",
    "                else:\n",
    "                    match[l].append(0)\n",
    "\n",
    "    for iter_ in (\n",
    "        pred_bboxes,\n",
    "        pred_labels,\n",
    "        pred_scores,\n",
    "        gt_bboxes,\n",
    "        gt_labels,\n",
    "        gt_difficults,\n",
    "    ):\n",
    "        if next(iter_, None) is not None:\n",
    "            raise ValueError(\"Length of input iterables need to be same.\")\n",
    "\n",
    "    n_fg_class = max(n_pos.keys()) + 1\n",
    "    prec = [None] * n_fg_class\n",
    "    rec = [None] * n_fg_class\n",
    "\n",
    "    for l in n_pos.keys():\n",
    "        score_l = np.array(score[l])\n",
    "        match_l = np.array(match[l], dtype=np.int8)\n",
    "\n",
    "        order = score_l.argsort()[::-1]\n",
    "        match_l = match_l[order]\n",
    "\n",
    "        tp = np.cumsum(match_l == 1)\n",
    "        fp = np.cumsum(match_l == 0)\n",
    "\n",
    "        # If an element of fp + tp is 0,\n",
    "        # the corresponding element of prec[l] is nan.\n",
    "        prec[l] = tp / (fp + tp)\n",
    "        # If n_pos[l] is 0, rec[l] is None.\n",
    "        if n_pos[l] > 0:\n",
    "            rec[l] = tp / n_pos[l]\n",
    "\n",
    "    return prec, rec\n",
    "\n",
    "\n",
    "def calc_detection_voc_ap(prec, rec, use_07_metric=False):\n",
    "    \"\"\"Calculate average precisions based on evaluation code of PASCAL VOC.\n",
    "\n",
    "    This function calculates average precisions\n",
    "    from given precisions and recalls.\n",
    "    The code is based on the evaluation code used in PASCAL VOC Challenge.\n",
    "\n",
    "    Args:\n",
    "        prec (list of numpy.array): A list of arrays.\n",
    "            :obj:`prec[l]` indicates precision for class :math:`l`.\n",
    "            If :obj:`prec[l]` is :obj:`None`, this function returns\n",
    "            :obj:`numpy.nan` for class :math:`l`.\n",
    "        rec (list of numpy.array): A list of arrays.\n",
    "            :obj:`rec[l]` indicates recall for class :math:`l`.\n",
    "            If :obj:`rec[l]` is :obj:`None`, this function returns\n",
    "            :obj:`numpy.nan` for class :math:`l`.\n",
    "        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n",
    "            for calculating average precision. The default value is\n",
    "            :obj:`False`.\n",
    "\n",
    "    Returns:\n",
    "        ~numpy.ndarray:\n",
    "        This function returns an array of average precisions.\n",
    "        The :math:`l`-th value corresponds to the average precision\n",
    "        for class :math:`l`. If :obj:`prec[l]` or :obj:`rec[l]` is\n",
    "        :obj:`None`, the corresponding value is set to :obj:`numpy.nan`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_fg_class = len(prec)\n",
    "    ap = np.empty(n_fg_class)\n",
    "    for l in six.moves.range(n_fg_class):\n",
    "        if prec[l] is None or rec[l] is None:\n",
    "            ap[l] = np.nan\n",
    "            continue\n",
    "\n",
    "        if use_07_metric:\n",
    "            # 11 point metric\n",
    "            ap[l] = 0\n",
    "            for t in np.arange(0.0, 1.1, 0.1):\n",
    "                if np.sum(rec[l] >= t) == 0:\n",
    "                    p = 0\n",
    "                else:\n",
    "                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n",
    "                ap[l] += p / 11\n",
    "        else:\n",
    "            # correct AP calculation\n",
    "            # first append sentinel values at the end\n",
    "            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n",
    "            mrec = np.concatenate(([0], rec[l], [1]))\n",
    "\n",
    "            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
    "\n",
    "            # to calculate area under PR curve, look for points\n",
    "            # where X axis (recall) changes value\n",
    "            i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "            # and sum (\\Delta recall) * prec\n",
    "            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data01/dl23vitcas/dl_project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config/single_task_object_detection.yaml\n",
      "models/object_detection/model_2024-06-23_11-20.pth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"view\",\n",
    "    \"--config\",\n",
    "    \"config/single_task_object_detection.yaml\",\n",
    "    \"--model_path\",\n",
    "    \"models/object_detection/model_2024-06-23_11-20.pth\",\n",
    "]\n",
    "\n",
    "# Creazione del parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "parser.add_argument(\n",
    "    \"--model_path\", type=str, required=True, help=\"Path to the model file\"\n",
    ")\n",
    "\n",
    "# Parsing degli argomenti\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Stampare i valori degli argomenti\n",
    "print(args.config)\n",
    "print(args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config_experiments import config, parse_args\n",
    "from utils import set_seed, set_device\n",
    "from dataloader import VOC08Attr\n",
    "import torchvision.transforms as transforms\n",
    "from model import ObjectDetectionModel\n",
    "from metrics import compute_mAP, view_mAP_for_class\n",
    "import wandb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    wandb.init(\n",
    "        group=\"object_detection\",\n",
    "        project=\"DL\",\n",
    "        config=config,\n",
    "        save_code=True,\n",
    "        mode=\"disabled\",\n",
    "    )\n",
    "    model_path = parse_args().model_path\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=config[\"transform\"][\"resize_values\"]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=config[\"transform\"][\"mean\"], std=config[\"transform\"][\"std\"]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    set_seed(config[\"global\"][\"seed\"])\n",
    "    device = set_device(config[\"global\"][\"gpu_id\"])\n",
    "    data_test = VOC08Attr(train=False, transform=transform_test)\n",
    "    model = ObjectDetectionModel().to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = compute_mAP(data_test, model, device)\n",
    "view_mAP_for_class(mAP, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_mAP_for_class(mAP, data_test):\n",
    "    print(mAP)\n",
    "    print(f\"\\nmAP@0.50 (per class):\")\n",
    "    index = torch.arange(1, config[\"global\"][\"num_classes\"] + 1)\n",
    "\n",
    "    for i, value in zip(index, mAP[\"map_per_class\"].numpy()):\n",
    "        category = data_test.id2category.get(i.item())\n",
    "        mAP_category = value.item()\n",
    "\n",
    "        print(f\"\\tAP {category} : {(mAP_category):.2f}\")\n",
    "        wandb.config.update({f\"AP {category} \": mAP_category})\n",
    "\n",
    "    mAP50 = mAP[\"map_50\"].item()\n",
    "    print(f\"\\nmAP@0.50 : {mAP50:.2f}\")\n",
    "\n",
    "    wandb.config.update({\"mAP@0.50\": mAP50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_mAP_for_class(mAP, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config_experiments import config\n",
    "from bbox_transform import resize_bounding_boxes\n",
    "import torchmetrics\n",
    "from bbox_transform import apply_nms\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import logging\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "def compute_mAP(data_set, model, device):  # train/val\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with open(\n",
    "        os.getcwd()\n",
    "        + \"/src/single_task_object_detection/\"\n",
    "        + \"target_mean_std_by_class.yaml\",\n",
    "        \"r\",\n",
    "    ) as f:\n",
    "        mean_std_by_class = yaml.safe_load(f)\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (\n",
    "            image,\n",
    "            image_size,\n",
    "            gt_class,\n",
    "            gt_bbox,\n",
    "            gt_attributes,\n",
    "            ss_rois,\n",
    "        ) in enumerate(tqdm(data_set, desc=\"Compute mAP\")):\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            gt_class = gt_class.to(device)\n",
    "            gt_bbox = gt_bbox.to(device)\n",
    "            ss_rois = ss_rois.to(device)\n",
    "\n",
    "            orig_w, orig_h = image_size\n",
    "            new_w, new_h = (image.shape[3], image.shape[2])\n",
    "            gt_bbox = resize_bounding_boxes(\n",
    "                gt_bbox, orig_size=(new_w, new_h), new_size=(orig_w, orig_h)\n",
    "            )\n",
    "\n",
    "            indices_batch = data_set.get_indices_batch(\n",
    "                image.shape[0], ss_rois.shape[0]\n",
    "            ).unsqueeze(-1)\n",
    "\n",
    "            indices_batch = indices_batch.to(device)\n",
    "\n",
    "            cls_max_score_net, max_score_net, bboxs_net = model.prediction_img(\n",
    "                image, ss_rois, indices_batch, mean_std_by_class\n",
    "            )\n",
    "\n",
    "            bboxs_net = resize_bounding_boxes(\n",
    "                bboxs_net, orig_size=(new_w, new_h), new_size=(orig_w, orig_h)\n",
    "            )\n",
    "\n",
    "            pred_bbox, pred_class, pred_score = apply_nms(\n",
    "                cls_max_score_net, max_score_net, bboxs_net\n",
    "            )\n",
    "\n",
    "            pred_bboxes.append(pred_bbox)\n",
    "            pred_labels.append(pred_class)\n",
    "            pred_scores.append(pred_score)\n",
    "            gt_bboxes.append(gt_bbox)\n",
    "            gt_labels.append(gt_class)\n",
    "    mAP = voc_eval(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute mAP:   0%|          | 0/2227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute mAP: 100%|██████████| 2227/2227 [09:16<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels = compute_mAP(\n",
    "    data_test, model, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ap': array([       nan, 0.36184114, 0.39342054, 0.15304123, 0.28581046,\n",
       "        0.43998192, 0.33640001, 0.46924918, 0.46955405, 0.13036021,\n",
       "        0.13498852, 0.29672787, 0.1849298 , 0.44415412, 0.45414605,\n",
       "        0.19410411, 0.2370254 , 0.11723687, 0.42764903, 0.21635322,\n",
       "        0.2183466 ]),\n",
       " 'map': 0.29826601751400483}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
