{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()\n",
    "\n",
    "sys.argv = [\"view\", \"--config\", \"config/single_task_object_detection.yaml\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_experiments import config\n",
    "from torchvision.transforms import transforms\n",
    "from dataloader import VOC08Attr\n",
    "import matplotlib.pyplot as plt\n",
    "from model import ObjectDetectionModel\n",
    "from utils import set_device\n",
    "import torch\n",
    "from bbox_transform import resize_bounding_boxes, apply_nms\n",
    "import matplotlib.patches as patches\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            size=config[\"transform\"][\"resize_values\"],\n",
    "            max_size=config[\"transform\"][\"max_size\"],\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=config[\"transform\"][\"mean\"], std=config[\"transform\"][\"std\"]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../dl_project/experiments/object_detection/2024-07-25_10-51-08/models/best_model_epoch_95.pth\"\n",
    "\n",
    "device = set_device(config[\"global\"][\"gpu_id\"])\n",
    "model = ObjectDetectionModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = VOC08Attr(train=False, transform=None)\n",
    "val_data_for_model = VOC08Attr(train=False, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(idx):\n",
    "    img_transform, img_size_orig_transform, _, _, _, ss_rois_transform = (\n",
    "        val_data_for_model[idx]\n",
    "    )\n",
    "    image, img_size_orig, gt_class, gt_bbox, gt_attributes, ss_rois = val_data[idx]\n",
    "    img_transform = img_transform.unsqueeze(0).to(device)\n",
    "    ss_rois_transform = ss_rois_transform.to(device)\n",
    "\n",
    "    indices_batch = torch.zeros(ss_rois_transform.shape[0], device=device).unsqueeze(-1)\n",
    "\n",
    "    cls_max_score_net, max_score_net, bboxs_net = model.prediction_img(\n",
    "        img_transform, ss_rois_transform, indices_batch\n",
    "    )\n",
    "\n",
    "    bboxs_net = resize_bounding_boxes(\n",
    "        bboxs_net,\n",
    "        orig_size=(img_transform.shape[3], img_transform.shape[2]),\n",
    "        new_size=img_size_orig_transform,\n",
    "    )\n",
    "\n",
    "    pred_bbox, pred_class, pred_score = apply_nms(\n",
    "        cls_max_score_net, max_score_net, bboxs_net\n",
    "    )\n",
    "    pred_bbox, pred_class, pred_score = (\n",
    "        pred_bbox.cpu(),\n",
    "        pred_class.cpu(),\n",
    "        pred_score.cpu(),\n",
    "    )\n",
    "    return image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score):\n",
    "    im = image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(im)\n",
    "\n",
    "    for gt_el in gt_bbox:\n",
    "        x_min, y_min, x_max, y_max = gt_el\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            x_max - x_min,\n",
    "            y_max - y_min,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"g\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    for pred_el in pred_bbox:\n",
    "        x_min, y_min, x_max, y_max = pred_el\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            x_max - x_min,\n",
    "            y_max - y_min,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    print(\"NET\")\n",
    "    for box, c, score in zip(pred_bbox, pred_class, pred_score):\n",
    "        print(f\"{box.int()} \\t class: {c.item()} \\tscore: {score.item():.3f}\")\n",
    "\n",
    "    print(\"\\nGT\")\n",
    "\n",
    "    for box, c in zip(gt_bbox, gt_class):\n",
    "        print(f\"{box.int()} \\t class: {c.item()}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(low=0, high=len(val_data), size=(1,))\n",
    "\n",
    "print(f\"IDX: {idx.item()}\\n\")\n",
    "image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score = inference(idx=idx)\n",
    "plot_inference(image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studio Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = []\n",
    "rec_list = []\n",
    "for idx in range(len(val_data)):\n",
    "    _, gt_bbox, gt_class, pred_bbox, pred_class, pred_score = inference(idx=idx)\n",
    "    ratio_list.append(\n",
    "        gt_bbox.shape[0] / max(pred_bbox.shape[0], gt_bbox.shape[0] * 0.01)\n",
    "    )\n",
    "    gt_bbox = gt_bbox.tolist()\n",
    "    pred_bbox = pred_bbox.tolist()\n",
    "    gt_class = gt_class.tolist()\n",
    "    pred_class = pred_class.tolist()\n",
    "    num_gt = len(gt_bbox)\n",
    "    i_pred = 0\n",
    "    while i_pred < len(pred_bbox):\n",
    "        i_gt = 0\n",
    "        while i_gt < len(gt_bbox):\n",
    "            iou = torchvision.ops.box_iou(\n",
    "                torch.tensor(pred_bbox[i_pred]).unsqueeze(0),\n",
    "                torch.tensor(gt_bbox[i_gt]).unsqueeze(0),\n",
    "            )\n",
    "            if iou.item() >= 0.5 and gt_class[i_gt] == pred_class[i_pred]:\n",
    "                gt_bbox.pop(i_gt)\n",
    "                gt_class.pop(i_gt)\n",
    "                pred_bbox.pop(i_pred)\n",
    "                pred_class.pop(i_pred)\n",
    "                i_pred -= 1\n",
    "                break\n",
    "            i_gt += 1\n",
    "        i_pred += 1\n",
    "        if len(gt_bbox) == 0:\n",
    "            break\n",
    "\n",
    "    rec_list.append((num_gt - len(gt_bbox)) / num_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = list(filter(lambda x: x < 100, ratio_list))\n",
    "_ = plt.hist(list(filter(lambda x: x < 2, ratio_list)), bins=20)\n",
    "print(f\"gt/pred {sum(ratios) / len(ratios)}\")\n",
    "print(f\"nopred/image{(len(ratio_list) - len(ratios)) / len(ratio_list)}\")\n",
    "print(f\"rec avg {sum(rec_list)/len(rec_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(rec_list, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import compute_mAP, view_mAP_for_class\n",
    "\n",
    "mAP = compute_mAP(val_data_for_model, model, device)\n",
    "mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP[\"map_per_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(mAP[\"map_per_class\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
