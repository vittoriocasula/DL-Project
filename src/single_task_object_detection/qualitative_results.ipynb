{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()\n",
    "\n",
    "sys.argv = [\"view\", \"--config\", \"config/single_task_object_detection.yaml\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", type=str, required=True, help=\"Path to the config file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_experiments import config\n",
    "from torchvision.transforms import transforms\n",
    "from dataloader import VOC08Attr\n",
    "import matplotlib.pyplot as plt\n",
    "from model import ObjectDetectionModel\n",
    "from utils import set_device\n",
    "import torch\n",
    "from bbox_transform import resize_bounding_boxes, apply_nms\n",
    "import matplotlib.patches as patches\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            size=config[\"transform\"][\"resize_values\"],\n",
    "            max_size=config[\"transform\"][\"max_size\"],\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=config[\"transform\"][\"mean\"], std=config[\"transform\"][\"std\"]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"../dl_project/experiments/object_detection/2024-07-25_10-51-08/models/best_model_epoch_95.pth\"\n",
    "model_path = \"../dl_project/experiments/object_detection/2024-07-28_19-23-43/models/model_epoch_75.pth\"\n",
    "\n",
    "device = set_device(config[\"global\"][\"gpu_id\"])\n",
    "model = ObjectDetectionModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = VOC08Attr(train=False, transform=None)\n",
    "val_data_for_model = VOC08Attr(train=False, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(idx):\n",
    "    img_transform, img_size_orig_transform, _, _, _, ss_rois_transform = (\n",
    "        val_data_for_model[idx]\n",
    "    )\n",
    "    image, img_size_orig, gt_class, gt_bbox, gt_attributes, ss_rois = val_data[idx]\n",
    "    img_transform = img_transform.unsqueeze(0).to(device)\n",
    "    ss_rois_transform = ss_rois_transform.to(device)\n",
    "\n",
    "    indices_batch = torch.zeros(ss_rois_transform.shape[0], device=device).unsqueeze(-1)\n",
    "\n",
    "    cls_max_score_net, max_score_net, bboxs_net = model.prediction_img(\n",
    "        img_transform, ss_rois_transform, indices_batch\n",
    "    )\n",
    "\n",
    "    bboxs_net = resize_bounding_boxes(\n",
    "        bboxs_net,\n",
    "        orig_size=(img_transform.shape[3], img_transform.shape[2]),\n",
    "        new_size=img_size_orig_transform,\n",
    "    )\n",
    "\n",
    "    pred_bbox, pred_class, pred_score = apply_nms(\n",
    "        cls_max_score_net, max_score_net, bboxs_net\n",
    "    )\n",
    "    pred_bbox, pred_class, pred_score = (\n",
    "        pred_bbox.cpu(),\n",
    "        pred_class.cpu(),\n",
    "        pred_score.cpu(),\n",
    "    )\n",
    "    return image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score):\n",
    "    im = image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(im)\n",
    "\n",
    "    for gt_el in gt_bbox:\n",
    "        x_min, y_min, x_max, y_max = gt_el\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            x_max - x_min,\n",
    "            y_max - y_min,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"g\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    for pred_el in pred_bbox:\n",
    "        x_min, y_min, x_max, y_max = pred_el\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            x_max - x_min,\n",
    "            y_max - y_min,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    print(\"NET\")\n",
    "    for box, c, score in zip(pred_bbox, pred_class, pred_score):\n",
    "        print(\n",
    "            f\"{box.int()} \\t class: {c.item()} ({val_data.id2category[c.item()]}) \\tscore: {score.item():.3f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nGT\")\n",
    "\n",
    "    for box, c in zip(gt_bbox, gt_class):\n",
    "        print(f\"{box.int()} \\t class: {c.item()} ({val_data.id2category[c.item()]})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(low=0, high=len(val_data), size=(1,))\n",
    "\n",
    "print(f\"IDX: {idx.item()}\\n\")\n",
    "image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score = inference(idx=idx)\n",
    "plot_inference(image, gt_bbox, gt_class, pred_bbox, pred_class, pred_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studio Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = []\n",
    "rec_list = []\n",
    "for idx in range(len(val_data)):\n",
    "    _, gt_bbox, gt_class, pred_bbox, pred_class, pred_score = inference(idx=idx)\n",
    "    ratio_list.append(\n",
    "        gt_bbox.shape[0] / max(pred_bbox.shape[0], gt_bbox.shape[0] * 0.01)\n",
    "    )\n",
    "    gt_bbox = gt_bbox.tolist()\n",
    "    pred_bbox = pred_bbox.tolist()\n",
    "    gt_class = gt_class.tolist()\n",
    "    pred_class = pred_class.tolist()\n",
    "    num_gt = len(gt_bbox)\n",
    "    i_pred = 0\n",
    "    while i_pred < len(pred_bbox):\n",
    "        i_gt = 0\n",
    "        while i_gt < len(gt_bbox):\n",
    "            iou = torchvision.ops.box_iou(\n",
    "                torch.tensor(pred_bbox[i_pred]).unsqueeze(0),\n",
    "                torch.tensor(gt_bbox[i_gt]).unsqueeze(0),\n",
    "            )\n",
    "            if iou.item() >= 0.5 and gt_class[i_gt] == pred_class[i_pred]:\n",
    "                gt_bbox.pop(i_gt)\n",
    "                gt_class.pop(i_gt)\n",
    "                pred_bbox.pop(i_pred)\n",
    "                pred_class.pop(i_pred)\n",
    "                i_pred -= 1\n",
    "                break\n",
    "            i_gt += 1\n",
    "        i_pred += 1\n",
    "        if len(gt_bbox) == 0:\n",
    "            break\n",
    "\n",
    "    rec_list.append((num_gt - len(gt_bbox)) / num_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = list(filter(lambda x: x < 100, ratio_list))\n",
    "_ = plt.hist(list(filter(lambda x: x < 2, ratio_list)), bins=20)\n",
    "print(f\"gt/pred {sum(ratios) / len(ratios)}\")\n",
    "print(f\"nopred/image{(len(ratio_list) - len(ratios)) / len(ratio_list)}\")\n",
    "print(f\"rec avg {sum(rec_list)/len(rec_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(rec_list, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import compute_mAP, view_mAP_for_class\n",
    "\n",
    "mAP = compute_mAP(val_data_for_model, model, device)\n",
    "mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP[\"map_per_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(mAP[\"map_per_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import VGG16_Weights\n",
    "from config_experiments import config\n",
    "from bbox_transform import regr_to_bbox\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "        rawnet = torchvision.models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(rawnet.features.children())[:-1])\n",
    "\n",
    "        # Freezare il primo layer conv\n",
    "        self.features[0].weight.requires_grad = False\n",
    "        self.features[0].bias.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.features(input)\n",
    "\n",
    "\n",
    "class ROI_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ROI_Module, self).__init__()\n",
    "\n",
    "        self.roipool = torchvision.ops.RoIPool(\n",
    "            output_size=(7, 7),  # Cambiare a 7x7\n",
    "            spatial_scale=config[\"model\"][\"spatial_scale\"],\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(\n",
    "                512 * 7 * 7,  # Cambiato da 256 a 512 per VGG16\n",
    "                4096,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, features, rois, ridx):\n",
    "        idx_rois = torch.cat(\n",
    "            (ridx, rois), dim=-1\n",
    "        )  # create matrix with (batch_idx, rois(xyxy))\n",
    "        res = self.roipool(features, idx_rois)\n",
    "        res = res.view(res.size(0), -1)\n",
    "        feat = self.classifier(res)\n",
    "        return feat\n",
    "\n",
    "\n",
    "class ObjectDetectionHead(nn.Module):\n",
    "    def __init__(self, num_classes=config[\"global\"][\"num_classes\"]):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.cls_score = nn.Linear(4096, self.num_classes + 1)\n",
    "        self.bbox = nn.Linear(4096, 4 * (self.num_classes + 1))\n",
    "\n",
    "        nn.init.normal_(self.cls_score.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.bbox.weight, mean=0, std=0.001)\n",
    "\n",
    "        nn.init.constant_(self.cls_score.bias, 0)\n",
    "        nn.init.constant_(self.bbox.bias, 0)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        cls_score = self.cls_score(feat)\n",
    "        bbox = self.bbox(feat).view(-1, self.num_classes + 1, 4)\n",
    "        return cls_score, bbox\n",
    "\n",
    "\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.roi_module = ROI_Module()\n",
    "        self.obj_detect_head = ObjectDetectionHead(\n",
    "            num_classes=config[\"global\"][\"num_classes\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, rois, ridx):\n",
    "        out = self.backbone(x)\n",
    "        out = self.roi_module(out, rois, ridx)\n",
    "        cls_score, bbox = self.obj_detect_head(out)\n",
    "        return cls_score, bbox\n",
    "\n",
    "    def prediction_img(self, img, rois, ridx):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            score, tbbox = self(img, rois, ridx)\n",
    "        _, _, heigth, width = img.shape\n",
    "        score = nn.functional.softmax(score, dim=-1)\n",
    "        max_score, cls_max_score = torch.max(score, dim=-1)\n",
    "\n",
    "        bboxs = regr_to_bbox(rois, tbbox, (heigth, width))\n",
    "\n",
    "        bboxs = bboxs[torch.arange(cls_max_score.shape[0]), cls_max_score]\n",
    "        # bboxs = self.unnormalize_bbox(bboxs)\n",
    "        return cls_max_score, max_score, bboxs\n",
    "\n",
    "    def calc_loss(\n",
    "        self,\n",
    "        probs,\n",
    "        bbox,\n",
    "        labels,\n",
    "        gt_bbox,\n",
    "    ):\n",
    "        # cel = nn.CrossEntropyLoss(weight=self.get_class_weigths().to(labels.device), label_smoothing=0.1)\n",
    "        cel = nn.CrossEntropyLoss()\n",
    "\n",
    "        sl1 = nn.SmoothL1Loss(reduction=\"none\")\n",
    "        loss_sc = cel(probs, labels)\n",
    "\n",
    "        # gt_bbox = self.normalize_bbox(gt_bbox)\n",
    "        mask = (labels != 0).bool()\n",
    "        t_u = bbox[torch.arange(bbox.shape[0]), labels]\n",
    "        loss_loc = (\n",
    "            torch.sum(torch.sum(sl1(t_u[mask], gt_bbox[mask]), dim=1), dim=0)\n",
    "            / labels.shape[0]\n",
    "        )\n",
    "\n",
    "        loss_sc = config[\"loss\"][\"lmb_cls\"] * loss_sc\n",
    "        loss_loc = config[\"loss\"][\"lmb_loc\"] * loss_loc\n",
    "        loss = loss_sc + loss_loc\n",
    "        return loss, loss_sc, loss_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = set_device(config[\"global\"][\"gpu_id\"])\n",
    "model = ObjectDetectionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
